{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-fbeefwmQhQ",
    "outputId": "c209ec9d-20b5-48cd-dd52-fa3f419e1c44"
   },
   "outputs": [],
   "source": [
    " %pip install gym[accept-rom-license]\n",
    " %pip install ale-py\n",
    " %pip install gymnasium[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opFX02R82zhh",
    "outputId": "946d3757-bc7e-4268-c529-27f56199f6d4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Jhd1c76A-Ps",
    "outputId": "618b7869-817d-4925-9075-fec364c6c619"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "env = gym.make('ALE/Pong-v5', render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWDYNz92BBS4",
    "outputId": "8d2d7c28-e846-4cd7-c01d-7228c1495519"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "num_episodes = 5\n",
    "env = gym.make('ALE/Pong-v5', render_mode=\"rgb_array\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} reward: {episode_reward}\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGe5Hn20BFar"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_frame(frame, last_frame=None):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    gray_frame = cv2.resize(gray_frame, (84, 84))\n",
    "\n",
    "    gray_frame = gray_frame.astype(np.float32) / 255.0\n",
    "\n",
    "    if last_frame is not None:\n",
    "        last_frame = last_frame.astype(np.float32)\n",
    "        gray_frame = cv2.absdiff(gray_frame, last_frame)\n",
    "\n",
    "    return gray_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56yoLyHNspyk"
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nt01d2m0my3j"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"Adds experience to the buffer\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a batch of experiences from the buffer\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNRpRxJw2-0v"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/content/drive/MyDrive/dqn_checkpoints/'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(policy_net, target_net, optimizer, episode, filename=\"dqn_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode': episode\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(CHECKPOINT_PATH, filename))\n",
    "    print(f\"Checkpoint saved at episode {episode}.\")\n",
    "\n",
    "def load_checkpoint(filename=\"dqn_checkpoint.pth\"):\n",
    "    filepath = os.path.join(CHECKPOINT_PATH, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_episode = checkpoint['episode'] + 1\n",
    "        print(f\"Checkpoint loaded. Resuming from episode {start_episode}.\")\n",
    "        return start_episode\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-QtGNvg3DzV",
    "outputId": "489f6f01-c4a4-4d62-d70c-f1da22b72775"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_episodes = 930\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "target_update_interval = 10\n",
    "\n",
    "\n",
    "memory = ReplayBuffer(capacity=10000)\n",
    "episode_rewards = []\n",
    "\n",
    "action_space = env.action_space.n\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "policy_net = DQNetwork(action_space).to(device)\n",
    "target_net = DQNetwork(action_space).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "start_episode = load_checkpoint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MW6PqjfaBLGx",
    "outputId": "221841cf-58cb-4bcb-8698-7466ca8f2b71"
   },
   "outputs": [],
   "source": [
    "for episode in range(start_episode, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    last_frame = None\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        frame = preprocess_frame(obs, last_frame)\n",
    "        state = np.stack((frame, last_frame), axis=0) if last_frame is not None else np.stack((frame, frame), axis=0)\n",
    "        last_frame = frame\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_obs, reward, done, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        next_frame = preprocess_frame(next_obs, last_frame)\n",
    "        next_state = np.stack((next_frame, frame), axis=0)\n",
    "\n",
    "        memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "            states = torch.FloatTensor(np.array(states)).to(device).view(batch_size, 2, 84, 84)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            next_states = torch.FloatTensor(np.array(next_states)).to(device).view(batch_size, 2, 84, 84)\n",
    "            dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "            current_q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                max_next_q_values = target_net(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "            loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    print(f\"Episode {episode}: Reward = {episode_reward}\")\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        save_checkpoint(policy_net, target_net, optimizer, episode)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1lZeK2uK06U"
   },
   "outputs": [],
   "source": [
    "\n",
    "episode_rewards_checkpoint = {\n",
    "    'episode_rewards': episode_rewards\n",
    "}\n",
    "torch.save(episode_rewards_checkpoint, 'pong_episode_rewards.pth')\n",
    "print(\"Episode rewards saved separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "jtz1PEjfBuuE",
    "outputId": "391313c2-9658-44d7-8e7d-370d0acb3455"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 10\n",
    "mean_rewards = [np.mean(episode_rewards[i:i+n]) for i in range(len(episode_rewards) - n + 1)]\n",
    "\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(f\"Mean {n}-Episode Reward\")\n",
    "plt.title(\"DQN Performance on Pong-v0\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM9DuSrxkNiM"
   },
   "source": [
    "In the above graph, we observe that the mean reward gradually approaches zero as the number of episodes increases. Additionally, the variation around the mean reward decreases over time, as indicated by the narrowing spread in the graph toward the right. Based on these trends, we can extrapolate that, after a certain number of episodes, the reward curve will converge to a zero-reward baseline, indicating stabilization in the agent's performance.\n",
    "Due to ristriction in computational resources this was the maximum that i was able to plot."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
