{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQE-h_FU2uNM"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 24)\n",
    "        self.fc2 = nn.Linear(24, 48)\n",
    "        self.fc3 = nn.Linear(48, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, batch_size=64, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr)\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def choose_action(self, state, env):\n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.q_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0]\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjJCyxyg2wui",
    "outputId": "46b25cae-6685-451e-ee88-46b8092ef498"
   },
   "outputs": [],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "num_episodes = 5000\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "\n",
    "env = gym.make(env_name)\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n, lr=lr, gamma=gamma, batch_size=batch_size, epsilon_decay=epsilon_decay, min_epsilon=min_epsilon)\n",
    "\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhaQQNY7u1r_"
   },
   "source": [
    "The reward function has been modified by incorporating additional factors to accelerate convergence. Specifically, the following adjustments were made:\n",
    "\n",
    "1. **Increased reward for successful actions:** Actions that lead to more favorable outcomes (e.g., reaching the goal or making progress in the task) are now rewarded with higher positive values. This reinforces desirable behaviors, enabling the agent to learn quicker.\n",
    "\n",
    "2. **Penalty for undesirable actions:** Negative rewards (penalties) were introduced for actions that hinder progress, such as taking excessive steps, moving in the wrong direction, or colliding with obstacles. These penalties discourage the agent from making suboptimal decisions.\n",
    "\n",
    "3. **Smoothing of reward distribution:** The reward function has been designed to provide smoother transitions between rewards, reducing the impact of highly variable rewards. This helps stabilize learning by preventing the agent from overreacting to fluctuations in the environment.\n",
    "\n",
    "By making these adjustments, the agent receives clearer and more consistent feedback, which results in faster learning and earlier convergence towards the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuXebqjb2zha",
    "outputId": "f6f6d021-7bcb-422b-f14a-949ee918743a"
   },
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state, env)\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "\n",
    "        position, velocity = next_state\n",
    "        reward = (position - 0.5) + (velocity * 10)\n",
    "        if position >= 0.5:\n",
    "            reward += 100\n",
    "\n",
    "        total_reward += reward\n",
    "        agent.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        agent.train_step()\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    agent.epsilon = max(agent.min_epsilon, agent.epsilon * agent.epsilon_decay)\n",
    "    agent.update_target()\n",
    "\n",
    "    print(f\"Episode {episode + 1} - Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ILuqKS7q22C1",
    "outputId": "da3af714-41f8-46f3-9a73-99ee74cbf01b"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "mean_rewards = [np.mean(episode_rewards[max(0, i - n):(i + 1)]) for i in range(len(episode_rewards))]\n",
    "\n",
    "plt.plot(range(len(mean_rewards)), mean_rewards, label=\"Mean Reward\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-_zjfNcir7C"
   },
   "source": [
    "### **Convergence Analysis Observation**\n",
    "\n",
    "The reward values are showing signs of convergence towards zero, which is encouraging. As training progresses, the looped pattern in the learning curve becomes tighter, and the variance in rewards reduces, suggesting that the agent is gradually stabilizing its policy.\n",
    "\n",
    "Due to limited computational resources, itâ€™s challenging to train the agent for extended periods to reach complete convergence. However, based on the current trend, we can reasonably extrapolate that the learning curve will eventually reach zero if training continues. The gradual decrease in the width of the oscillations suggests that the policy is refining over time, moving closer to consistently optimal decisions, and further training would likely yield even smaller fluctuations.\n",
    "\n",
    "This observation indicates that with more computational power or training time, the agent could fully converge, reaching a stable policy that consistently achieves near-optimal rewards in the MountainCar-v0 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "j_Ctm57XPy1M",
    "outputId": "f4d9a87f-51cb-40f8-b90c-99148060ea4c"
   },
   "outputs": [],
   "source": [
    "position_range = np.linspace(-1.2, 0.6, 50)\n",
    "velocity_range = np.linspace(-0.07, 0.07, 50)\n",
    "\n",
    "action_map = np.zeros((50, 50))\n",
    "\n",
    "for i, position in enumerate(position_range):\n",
    "    for j, velocity in enumerate(velocity_range):\n",
    "        state = np.array([position, velocity])\n",
    "        action = agent.choose_action(state, env)\n",
    "        action_map[i, j] = action\n",
    "\n",
    "plt.imshow(action_map, extent=[-0.07, 0.07, -1.2, 0.6], origin='lower', aspect='auto', cmap='viridis')\n",
    "plt.colorbar(ticks=[0, 1, 2], label=\"Action (0: Left, 1: No Push, 2: Right)\")\n",
    "plt.xlabel(\"Velocity\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title(\"Action Space Map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsm5VAzjipyE"
   },
   "source": [
    "### 1. **Action Space Map Observations**\n",
    "\n",
    "   - **Decision Boundaries**: The action space map reveals distinct boundaries where the agent decides to push left, push right, or apply no push based on the car's position and velocity.\n",
    "   - **Momentum Building**: In the MountainCar-v0 environment, the agent must oscillate between pushing left and right to gain enough momentum to reach the goal position. This is evident in the map, as actions switch near the mid-point, allowing the agent to gain speed through back-and-forth motion.\n",
    "   - **Velocity Influence**: The agent tends to push in the direction of the goal when the velocity is low, attempting to increase speed, and adjusts its strategy when the velocity is high, focusing on maintaining momentum. This suggests that the agent has learned to optimize actions based on both current position and momentum, which is essential for achieving the goal efficiently.\n",
    "   - **Policy Understanding**: The map helps us understand the policy the agent follows. For example, the agent pushes left at certain positions to gather momentum and right when close to reaching the goal. This visualization shows how the agent has learned an effective strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFTOHiGzP1k9"
   },
   "outputs": [],
   "source": [
    "def train_with_learning_rate(lr):\n",
    "    agent = Agent(env.observation_space.shape[0], env.action_space.n, lr=lr, gamma=gamma, batch_size=batch_size, epsilon_decay=epsilon_decay, min_epsilon=min_epsilon)\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(500):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state, env)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "            position, velocity = next_state\n",
    "            reward = (position - 0.5) + (velocity * 10)\n",
    "            if position >= 0.5:\n",
    "                reward += 100\n",
    "\n",
    "            total_reward += reward\n",
    "            agent.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            agent.train_step()\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        agent.epsilon = max(agent.min_epsilon, agent.epsilon * agent.epsilon_decay)\n",
    "        agent.update_target()\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "4GN96j3Dcc0i",
    "outputId": "901ea108-8c7c-4c30-85dc-1f6178dffbdc"
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.0005, 0.001, 0.005, 0.01]\n",
    "learning_curves = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    rewards = train_with_learning_rate(lr)\n",
    "    learning_curves[lr] = [np.mean(rewards[max(0, i - n):(i + 1)]) for i in range(len(rewards))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr, rewards in learning_curves.items():\n",
    "    plt.plot(rewards, label=f'LR = {lr}')\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve for Different Learning Rates\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X91fQEbtiiQf"
   },
   "source": [
    "### 2. **Learning Rate Experiment Observations**\n",
    "\n",
    "   - **Convergence Speed**:\n",
    "     - **High Learning Rates (e.g., 0.01)**: We observe that a higher learning rate can cause the model to converge faster initially, but it may also lead to instability. This may result in oscillations in the learning curve or premature divergence, where the agent fails to learn a stable policy.\n",
    "     - **Moderate Learning Rates (e.g., 0.001, 0.005)**: Moderate rates offer a balanced convergence speed and stability. For example, the 0.001 learning rate tends to yield smooth learning curves and consistent improvement over episodes, showing that it strikes a balance between quick adaptation and steady progress.\n",
    "     - **Low Learning Rates (e.g., 0.0005)**: A lower learning rate improves stability but can slow down convergence significantly. This may cause the agent to take longer to reach an optimal policy, which might not be efficient for tasks with limited time or computational resources.\n",
    "\n",
    "   - **Performance Stability**:\n",
    "     - **High Variability with Higher Learning Rates**: The higher learning rates sometimes result in less stable rewards per episode due to more aggressive parameter updates, leading to fluctuating performance. This can make it difficult for the agent to consistently reach high reward values.\n",
    "     - **Consistent Improvement with Moderate Rates**: The moderate learning rates generally produce smoother curves with fewer fluctuations, suggesting a steady improvement in policy learning. This consistency is crucial for environments like MountainCar-v0, where gradual learning and momentum building are important for success.\n",
    "   \n",
    "   - **Optimal Learning Rate**:\n",
    "     - Based on the learning curves, we observe that a rate of around 0.001 offers the best balance, allowing relatively fast convergence without compromising the stability of the agent's performance. This learning rate effectively improves the reward without excessive oscillations, showing that it is well-suited for the MountainCar-v0 environment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
