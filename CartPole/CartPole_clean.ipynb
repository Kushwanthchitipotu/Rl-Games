{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-fbeefwmQhQ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmYlfY7Jt9YE",
    "outputId": "bae5f44b-5bbf-405c-ba84-657dc2662b0c"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"State Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "done, state = False, env.reset()[0]\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward obtained by random agent:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar56I4r0uXzH"
   },
   "source": [
    "\n",
    "1. **State and Action Spaces**:\n",
    "   - The **state space** in the `CartPole-v0` environment consists of continuous variables: the position and velocity of the cart, as well as the angle and angular velocity of the pole. These state variables describe the dynamics of the cart and the pole, which the agent must learn to balance.\n",
    "   - The **action space** is discrete, consisting of two actions: moving the cart to the left or right. These actions are crucial for the agent to balance the pole and prevent it from falling.\n",
    "\n",
    "2. **Random Agent Evaluation**:\n",
    "   - A random agent was used to explore the environment and understand its reward structure. The agent randomly selects actions (left or right) at each step, with the goal of keeping the pole balanced.\n",
    "   - The **total reward** obtained by the random agent is the sum of the rewards for each time step, reflecting how long the agent was able to keep the pole balanced before it fell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opFX02R82zhh"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.fc2(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Jhd1c76A-Ps"
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma, reward_to_go):\n",
    "    returns = []\n",
    "    if reward_to_go:\n",
    "        for t in range(len(rewards)):\n",
    "            Gt = sum([gamma ** (k - t) * rewards[k] for k in range(t, len(rewards))])\n",
    "            returns.append(Gt)\n",
    "    else:\n",
    "        G = sum([gamma ** t * r for t, r in enumerate(rewards)])\n",
    "        returns = [G] * len(rewards)\n",
    "    return returns\n",
    "def normalize_advantages(advantages):\n",
    "    advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "    return advantages\n",
    "def policy_gradient(env, policy, optimizer, gamma=0.99, reward_to_go=True, advantage_norm=True, num_iterations=1000, batch_size=500):\n",
    "    average_returns = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        states, actions, advantages = [], [], []\n",
    "        episode_rewards = []\n",
    "        episode_returns = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while len(states) < batch_size:\n",
    "\n",
    "            action_probs = policy(torch.tensor(state, dtype=torch.float32))\n",
    "            action = np.random.choice(len(action_probs), p=action_probs.detach().numpy())\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "\n",
    "                returns = compute_returns(episode_rewards, gamma, reward_to_go)\n",
    "                baseline = np.mean(returns)\n",
    "                episode_advantages = returns - baseline if advantage_norm else returns\n",
    "                if advantage_norm:\n",
    "                    episode_advantages = normalize_advantages(episode_advantages)\n",
    "\n",
    "                advantages.extend(episode_advantages)\n",
    "                episode_returns.append(sum(episode_rewards))\n",
    "\n",
    "                episode_rewards = []\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "\n",
    "\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(actions[:len(advantages)], dtype=torch.int64)\n",
    "        advantages_tensor = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = torch.log(policy(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze())\n",
    "        loss = -torch.mean(log_probs * advantages_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_return = np.mean(episode_returns)\n",
    "        average_returns.append(avg_return)\n",
    "        print(f\"Iteration {i + 1}/{num_iterations}, Average Return: {avg_return}\")\n",
    "\n",
    "    return average_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWDYNz92BBS4",
    "outputId": "adf9acf9-3e56-418f-b170-e6e58449ff15"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "policy = PolicyNetwork(env.observation_space.shape[0], hidden_dim=128, output_dim=env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "average_returns_no_rtgo_no_norm = policy_gradient(env, policy, optimizer, reward_to_go=False, advantage_norm=False, num_iterations=5000)\n",
    "average_returns_no_rtgo_norm = policy_gradient(env, policy, optimizer, reward_to_go=False, advantage_norm=True, num_iterations=5000)\n",
    "average_returns_rtgo_no_norm = policy_gradient(env, policy, optimizer, reward_to_go=True, advantage_norm=False, num_iterations=5000)\n",
    "average_returns_rtgo_norm = policy_gradient(env, policy, optimizer, reward_to_go=True, advantage_norm=True, num_iterations=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "SGe5Hn20BFar",
    "outputId": "38ff6143-c45d-4440-9208-e299a87769eb"
   },
   "outputs": [],
   "source": [
    "plt.plot(average_returns_no_rtgo_no_norm, label=\"No Reward-to-Go, No Normalization\")\n",
    "plt.plot(average_returns_no_rtgo_norm, label=\"No Reward-to-Go & Normalization\")\n",
    "plt.plot(average_returns_rtgo_no_norm, label=\"Reward-to-Go, No Normalization\")\n",
    "plt.plot(average_returns_rtgo_norm, label=\"Reward-to-Go & Normalization\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Return\")\n",
    "plt.legend()\n",
    "plt.title(\"Policy Gradient with Variance Reduction Techniques\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APrEl1VRtm5y"
   },
   "source": [
    "####  Observations on Variance Reduction Techniques\n",
    "\n",
    "The four configurations tested with variance reduction techniques yielded the following conclusions:\n",
    "\n",
    "1. **No Reward-to-Go, No Normalization**:\n",
    "   - This baseline configuration experienced high variance in returns and slow convergence. Without normalization or reward-to-go, the learning process was unstable, showing fluctuations in the returns over time.\n",
    "\n",
    "2. **No Reward-to-Go with Normalization**:\n",
    "   - Adding advantage normalization stabilized the learning process by reducing the variance of the gradient updates, leading to smoother returns. Convergence improved, though it was still slower than configurations using reward-to-go.\n",
    "\n",
    "3. **Reward-to-Go, No Normalization**:\n",
    "   - The reward-to-go technique improved stability by reducing variance in gradient estimates, as it allowed rewards to reflect immediate action results rather than the entire trajectory. This configuration had better convergence than both configurations without reward-to-go.\n",
    "\n",
    "4. **Reward-to-Go with Normalization**:\n",
    "   - Combining reward-to-go with advantage normalization produced the most stable and efficient learning process. The returns converged smoothly with minimal variance, making this configuration the most effective for maximizing average return over iterations.\n",
    "\n",
    "**Conclusion**:  \n",
    "**Reward-to-Go with Advantage Normalization** yielded the most stable and efficient learning, making it the preferred configuration. Using both techniques together helped in significantly reducing gradient variance and improving policy convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56yoLyHNspyk",
    "outputId": "951f5c9d-63f5-4a16-cd66-423904043051"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [200, 300, 400,500]\n",
    "all_returns = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nRunning policy gradient with batch size {batch_size}\")\n",
    "    returns = policy_gradient(env, policy, optimizer, reward_to_go=True, advantage_norm=True, num_iterations=1000, batch_size=batch_size)\n",
    "    all_returns[batch_size] = returns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "x9W8gWHdF6lq",
    "outputId": "ce90850f-8e5f-452c-fbe4-c71d656ec1b6"
   },
   "outputs": [],
   "source": [
    "for batch_size, returns in all_returns.items():\n",
    "    plt.plot(returns, label=f\"Batch Size {batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Return\")\n",
    "plt.legend()\n",
    "plt.title(\"Impact of Batch Size on Policy Gradient Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSxtSGtwtiIt"
   },
   "source": [
    "#### Impact of Batch Size on Policy Gradient Performance\n",
    "\n",
    "Batch sizes of 200, 300, 400, and 500 were tested to analyze their impact on the performance of the policy gradient algorithm.\n",
    "\n",
    "1. **Batch Size = 200**:\n",
    "   - Provided quick updates and faster convergence, but with a high variance in returns due to smaller sample sizes per update. Learning was less stable, with noticeable fluctuations in performance.\n",
    "\n",
    "2. **Batch Size = 300**:\n",
    "   - Showed a good balance between stability and convergence speed. The returns were smoother, with reduced variance compared to batch size 200, and convergence was efficient.\n",
    "\n",
    "3. **Batch Size = 400**:\n",
    "   - Achieved greater stability with even smoother returns. Convergence was slower than batch sizes 200 and 300, but the stability made learning more reliable.\n",
    "\n",
    "4. **Batch Size = 500**:\n",
    "   - This batch size offered the most stable learning curve with minimal fluctuations, but convergence was slower due to less frequent updates. Stability was maximized, but the high batch size made the algorithm update less often, leading to slower overall learning.\n",
    "\n",
    "**Conclusion**:  \n",
    "**Batch sizes of 300 and 400** provided the best trade-offs, balancing stability and convergence speed. Larger batch sizes (400–500) increased stability but resulted in slower updates, while smaller batch sizes (200) converged faster but showed high variance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
